{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b7\u001b[1A\u001b[1G\u001b[27G[Files: 0  Bytes: 0  [0 B/s] Re]\u001b8\u001b7\u001b[2A\u001b[1G\u001b[27G[https://raw.githubusercontent.]\u001b8\u001b7\u001b[2A\u001b[1Ginput.txt.5            3% [>                             ]   15.06K    --.-KB/s\u001b8\u001b7\u001b[2A\u001b[1Ginput.txt.5          100% [=============================>]  424.87K    4.16MB/s\u001b8\u001b7\u001b[1A\u001b[1G\u001b[27G[Files: 1  Bytes: 424.87K [501.]\u001b8\u001b[m\u001b[m\u001b[m\u001b[m"
     ]
    }
   ],
   "source": [
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi, I am Pranav'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(\"Hi, I am Pranav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "torch.tensor(encode(\"Hi, I am Pranav\"))\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 32\n",
    "BATCH_SIZE = 8\n",
    "eval_interval = 20\n",
    "max_iters = 10000\n",
    "eval_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6d382715f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 32]),\n",
       " tensor([[58, 53,  1, 41, 53, 56, 56, 59, 54, 58,  1, 39,  1, 51, 39, 52,  5, 57,\n",
       "           1, 61, 47, 44, 43,  1, 47, 57,  0, 61, 46, 43, 52,  1],\n",
       "         [49,  1, 39, 52,  1, 53, 39, 58, 46,  1, 40, 63,  1, 20, 47, 51,  6,  0,\n",
       "          32, 46, 43,  1, 59, 52, 47, 58, 63,  1, 58, 46, 43,  1],\n",
       "         [59, 50, 42,  1, 58, 46, 53, 59,  1, 61, 43, 56, 58,  1, 57, 53,  1, 58,\n",
       "          53, 53,  2,  0,  0, 24, 33, 15, 21, 27, 10,  0, 35, 43],\n",
       "         [ 8,  0,  0, 35, 13, 30, 35, 21, 15, 23, 10,  0, 28, 56, 53, 60, 43,  1,\n",
       "          47, 58,  6,  1, 20, 43, 52, 56, 63,  6,  1, 39, 52, 42],\n",
       "         [58,  1, 57, 46, 43,  8,  0,  0, 32, 30, 13, 26, 21, 27, 10,  0, 18, 53,\n",
       "          56,  1, 61, 46, 39, 58,  1, 56, 43, 39, 57, 53, 52,  6],\n",
       "         [56, 61, 47, 41, 49,  6,  1, 50, 43, 58,  1, 47, 58,  1, 40, 43, 11,  0,\n",
       "          18, 53, 56,  1, 47, 52,  1, 58, 46, 63,  1, 57, 46, 53],\n",
       "         [25, 10,  0, 35, 47, 58, 46, 42, 56, 39, 61,  1, 63, 53, 59,  1, 46, 43,\n",
       "          52, 41, 43,  6,  1, 51, 63,  1, 50, 53, 56, 42,  6,  1],\n",
       "         [43, 57, 58,  1, 61, 47, 58, 46,  1, 58, 46, 63,  1, 44, 56, 53, 64, 43,\n",
       "          52,  1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52,  0, 25]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "xb.shape, xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor([[-0.2622,  1.7675, -0.6312,  ...,  0.3641, -0.8859, -0.6271],\n",
      "        [ 0.5739,  0.6314,  0.8301,  ...,  0.0139,  0.0031, -0.3233],\n",
      "        [-1.9560, -0.8003, -0.5045,  ..., -1.5630,  1.1346, -0.0824],\n",
      "        ...,\n",
      "        [-1.4726,  1.1064, -0.0257,  ..., -0.2800, -0.0915,  0.4282],\n",
      "        [-0.5024, -0.2026, -1.5671,  ...,  0.1108,  0.3925,  0.8353],\n",
      "        [ 0.2433, -0.1318, -0.6623,  ..., -1.8573,  0.1714, -1.0309]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "&rEnLTjLDJIcLVR'JIHDTHdhsV\n",
      "v\n",
      "wxh,nhUYZzAEOZHpgo3q3ZYZes$zuGw,;eMk QqACRfCLgxiW3.O!zDLgA YsTb!dHb!;pK\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(logits)\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7053, val loss 4.7108\n",
      "step 20: train loss 4.6721, val loss 4.6900\n",
      "step 40: train loss 4.6539, val loss 4.6642\n",
      "step 60: train loss 4.6386, val loss 4.6436\n",
      "step 80: train loss 4.6073, val loss 4.6112\n",
      "step 100: train loss 4.5850, val loss 4.6017\n",
      "step 120: train loss 4.5667, val loss 4.5692\n",
      "step 140: train loss 4.5350, val loss 4.5550\n",
      "step 160: train loss 4.5159, val loss 4.5371\n",
      "step 180: train loss 4.5007, val loss 4.5133\n",
      "step 200: train loss 4.4696, val loss 4.4941\n",
      "step 220: train loss 4.4498, val loss 4.4634\n",
      "step 240: train loss 4.4321, val loss 4.4425\n",
      "step 260: train loss 4.4067, val loss 4.4314\n",
      "step 280: train loss 4.3935, val loss 4.4021\n",
      "step 300: train loss 4.3649, val loss 4.3915\n",
      "step 320: train loss 4.3575, val loss 4.3612\n",
      "step 340: train loss 4.3214, val loss 4.3409\n",
      "step 360: train loss 4.3143, val loss 4.3125\n",
      "step 380: train loss 4.2870, val loss 4.3090\n",
      "step 400: train loss 4.2636, val loss 4.2871\n",
      "step 420: train loss 4.2428, val loss 4.2653\n",
      "step 440: train loss 4.2327, val loss 4.2346\n",
      "step 460: train loss 4.2161, val loss 4.2252\n",
      "step 480: train loss 4.1876, val loss 4.1973\n",
      "step 500: train loss 4.1600, val loss 4.1874\n",
      "step 520: train loss 4.1525, val loss 4.1689\n",
      "step 540: train loss 4.1317, val loss 4.1508\n",
      "step 560: train loss 4.1179, val loss 4.1398\n",
      "step 580: train loss 4.1000, val loss 4.1123\n",
      "step 600: train loss 4.0827, val loss 4.1022\n",
      "step 620: train loss 4.0574, val loss 4.0787\n",
      "step 640: train loss 4.0393, val loss 4.0568\n",
      "step 660: train loss 4.0270, val loss 4.0406\n",
      "step 680: train loss 3.9990, val loss 4.0239\n",
      "step 700: train loss 3.9769, val loss 3.9943\n",
      "step 720: train loss 3.9788, val loss 3.9925\n",
      "step 740: train loss 3.9585, val loss 3.9728\n",
      "step 760: train loss 3.9402, val loss 3.9420\n",
      "step 780: train loss 3.9220, val loss 3.9281\n",
      "step 800: train loss 3.9026, val loss 3.9222\n",
      "step 820: train loss 3.8816, val loss 3.9024\n",
      "step 840: train loss 3.8714, val loss 3.8807\n",
      "step 860: train loss 3.8435, val loss 3.8711\n",
      "step 880: train loss 3.8425, val loss 3.8545\n",
      "step 900: train loss 3.8197, val loss 3.8345\n",
      "step 920: train loss 3.8133, val loss 3.8259\n",
      "step 940: train loss 3.7822, val loss 3.8176\n",
      "step 960: train loss 3.7763, val loss 3.7880\n",
      "step 980: train loss 3.7596, val loss 3.7856\n",
      "step 1000: train loss 3.7461, val loss 3.7723\n",
      "step 1020: train loss 3.7375, val loss 3.7622\n",
      "step 1040: train loss 3.7159, val loss 3.7369\n",
      "step 1060: train loss 3.7068, val loss 3.7255\n",
      "step 1080: train loss 3.6845, val loss 3.7046\n",
      "step 1100: train loss 3.6708, val loss 3.6944\n",
      "step 1120: train loss 3.6598, val loss 3.6668\n",
      "step 1140: train loss 3.6476, val loss 3.6636\n",
      "step 1160: train loss 3.6279, val loss 3.6500\n",
      "step 1180: train loss 3.6326, val loss 3.6369\n",
      "step 1200: train loss 3.6035, val loss 3.6224\n",
      "step 1220: train loss 3.5919, val loss 3.6118\n",
      "step 1240: train loss 3.5668, val loss 3.5962\n",
      "step 1260: train loss 3.5742, val loss 3.5851\n",
      "step 1280: train loss 3.5489, val loss 3.5628\n",
      "step 1300: train loss 3.5309, val loss 3.5541\n",
      "step 1320: train loss 3.5176, val loss 3.5413\n",
      "step 1340: train loss 3.5191, val loss 3.5383\n",
      "step 1360: train loss 3.5009, val loss 3.5229\n",
      "step 1380: train loss 3.4896, val loss 3.5005\n",
      "step 1400: train loss 3.4725, val loss 3.4945\n",
      "step 1420: train loss 3.4770, val loss 3.4912\n",
      "step 1440: train loss 3.4542, val loss 3.4734\n",
      "step 1460: train loss 3.4448, val loss 3.4527\n",
      "step 1480: train loss 3.4295, val loss 3.4500\n",
      "step 1500: train loss 3.4138, val loss 3.4446\n",
      "step 1520: train loss 3.4050, val loss 3.4257\n",
      "step 1540: train loss 3.3886, val loss 3.4075\n",
      "step 1560: train loss 3.3866, val loss 3.4152\n",
      "step 1580: train loss 3.3739, val loss 3.3935\n",
      "step 1600: train loss 3.3537, val loss 3.3786\n",
      "step 1620: train loss 3.3542, val loss 3.3649\n",
      "step 1640: train loss 3.3379, val loss 3.3516\n",
      "step 1660: train loss 3.3353, val loss 3.3461\n",
      "step 1680: train loss 3.3244, val loss 3.3377\n",
      "step 1700: train loss 3.3147, val loss 3.3238\n",
      "step 1720: train loss 3.2974, val loss 3.3194\n",
      "step 1740: train loss 3.2791, val loss 3.3020\n",
      "step 1760: train loss 3.2692, val loss 3.2843\n",
      "step 1780: train loss 3.2684, val loss 3.2845\n",
      "step 1800: train loss 3.2548, val loss 3.2651\n",
      "step 1820: train loss 3.2470, val loss 3.2637\n",
      "step 1840: train loss 3.2416, val loss 3.2472\n",
      "step 1860: train loss 3.2266, val loss 3.2415\n",
      "step 1880: train loss 3.2099, val loss 3.2388\n",
      "step 1900: train loss 3.2100, val loss 3.2349\n",
      "step 1920: train loss 3.2063, val loss 3.2137\n",
      "step 1940: train loss 3.1920, val loss 3.2038\n",
      "step 1960: train loss 3.1742, val loss 3.2055\n",
      "step 1980: train loss 3.1749, val loss 3.1878\n",
      "step 2000: train loss 3.1671, val loss 3.1861\n",
      "step 2020: train loss 3.1502, val loss 3.1783\n",
      "step 2040: train loss 3.1547, val loss 3.1652\n",
      "step 2060: train loss 3.1352, val loss 3.1567\n",
      "step 2080: train loss 3.1308, val loss 3.1494\n",
      "step 2100: train loss 3.1313, val loss 3.1394\n",
      "step 2120: train loss 3.1136, val loss 3.1343\n",
      "step 2140: train loss 3.1052, val loss 3.1155\n",
      "step 2160: train loss 3.1029, val loss 3.1094\n",
      "step 2180: train loss 3.0942, val loss 3.1056\n",
      "step 2200: train loss 3.0847, val loss 3.0974\n",
      "step 2220: train loss 3.0686, val loss 3.0818\n",
      "step 2240: train loss 3.0759, val loss 3.0899\n",
      "step 2260: train loss 3.0517, val loss 3.0776\n",
      "step 2280: train loss 3.0594, val loss 3.0695\n",
      "step 2300: train loss 3.0401, val loss 3.0627\n",
      "step 2320: train loss 3.0240, val loss 3.0533\n",
      "step 2340: train loss 3.0326, val loss 3.0424\n",
      "step 2360: train loss 3.0289, val loss 3.0473\n",
      "step 2380: train loss 3.0069, val loss 3.0372\n",
      "step 2400: train loss 3.0029, val loss 3.0148\n",
      "step 2420: train loss 2.9970, val loss 3.0166\n",
      "step 2440: train loss 3.0018, val loss 3.0021\n",
      "step 2460: train loss 2.9901, val loss 3.0057\n",
      "step 2480: train loss 2.9753, val loss 2.9903\n",
      "step 2500: train loss 2.9874, val loss 2.9821\n",
      "step 2520: train loss 2.9632, val loss 2.9772\n",
      "step 2540: train loss 2.9674, val loss 2.9791\n",
      "step 2560: train loss 2.9551, val loss 2.9651\n",
      "step 2580: train loss 2.9576, val loss 2.9576\n",
      "step 2600: train loss 2.9474, val loss 2.9518\n",
      "step 2620: train loss 2.9325, val loss 2.9638\n",
      "step 2640: train loss 2.9254, val loss 2.9446\n",
      "step 2660: train loss 2.9281, val loss 2.9404\n",
      "step 2680: train loss 2.9212, val loss 2.9433\n",
      "step 2700: train loss 2.9179, val loss 2.9231\n",
      "step 2720: train loss 2.9132, val loss 2.9280\n",
      "step 2740: train loss 2.9034, val loss 2.9110\n",
      "step 2760: train loss 2.8888, val loss 2.9128\n",
      "step 2780: train loss 2.8944, val loss 2.9025\n",
      "step 2800: train loss 2.8896, val loss 2.8935\n",
      "step 2820: train loss 2.8769, val loss 2.8929\n",
      "step 2840: train loss 2.8618, val loss 2.8857\n",
      "step 2860: train loss 2.8594, val loss 2.8884\n",
      "step 2880: train loss 2.8580, val loss 2.8687\n",
      "step 2900: train loss 2.8520, val loss 2.8717\n",
      "step 2920: train loss 2.8486, val loss 2.8520\n",
      "step 2940: train loss 2.8488, val loss 2.8657\n",
      "step 2960: train loss 2.8314, val loss 2.8583\n",
      "step 2980: train loss 2.8331, val loss 2.8542\n",
      "step 3000: train loss 2.8297, val loss 2.8410\n",
      "step 3020: train loss 2.8344, val loss 2.8324\n",
      "step 3040: train loss 2.8192, val loss 2.8417\n",
      "step 3060: train loss 2.8121, val loss 2.8305\n",
      "step 3080: train loss 2.8104, val loss 2.8193\n",
      "step 3100: train loss 2.8048, val loss 2.8226\n",
      "step 3120: train loss 2.8132, val loss 2.8255\n",
      "step 3140: train loss 2.7929, val loss 2.8135\n",
      "step 3160: train loss 2.7983, val loss 2.7959\n",
      "step 3180: train loss 2.7952, val loss 2.8040\n",
      "step 3200: train loss 2.7856, val loss 2.8047\n",
      "step 3220: train loss 2.7776, val loss 2.8007\n",
      "step 3240: train loss 2.7749, val loss 2.8039\n",
      "step 3260: train loss 2.7796, val loss 2.7798\n",
      "step 3280: train loss 2.7723, val loss 2.7860\n",
      "step 3300: train loss 2.7553, val loss 2.7718\n",
      "step 3320: train loss 2.7547, val loss 2.7701\n",
      "step 3340: train loss 2.7664, val loss 2.7651\n",
      "step 3360: train loss 2.7561, val loss 2.7700\n",
      "step 3380: train loss 2.7550, val loss 2.7785\n",
      "step 3400: train loss 2.7528, val loss 2.7533\n",
      "step 3420: train loss 2.7488, val loss 2.7610\n",
      "step 3440: train loss 2.7370, val loss 2.7508\n",
      "step 3460: train loss 2.7364, val loss 2.7395\n",
      "step 3480: train loss 2.7420, val loss 2.7385\n",
      "step 3500: train loss 2.7356, val loss 2.7553\n",
      "step 3520: train loss 2.7256, val loss 2.7317\n",
      "step 3540: train loss 2.7130, val loss 2.7319\n",
      "step 3560: train loss 2.7166, val loss 2.7322\n",
      "step 3580: train loss 2.7311, val loss 2.7270\n",
      "step 3600: train loss 2.7166, val loss 2.7311\n",
      "step 3620: train loss 2.7060, val loss 2.7170\n",
      "step 3640: train loss 2.6934, val loss 2.7162\n",
      "step 3660: train loss 2.7058, val loss 2.7198\n",
      "step 3680: train loss 2.6971, val loss 2.7031\n",
      "step 3700: train loss 2.6937, val loss 2.7134\n",
      "step 3720: train loss 2.6797, val loss 2.6970\n",
      "step 3740: train loss 2.6803, val loss 2.7077\n",
      "step 3760: train loss 2.6825, val loss 2.7064\n",
      "step 3780: train loss 2.6736, val loss 2.6923\n",
      "step 3800: train loss 2.6806, val loss 2.6952\n",
      "step 3820: train loss 2.6849, val loss 2.6924\n",
      "step 3840: train loss 2.6797, val loss 2.6808\n",
      "step 3860: train loss 2.6745, val loss 2.6885\n",
      "step 3880: train loss 2.6702, val loss 2.6918\n",
      "step 3900: train loss 2.6651, val loss 2.6727\n",
      "step 3920: train loss 2.6678, val loss 2.6726\n",
      "step 3940: train loss 2.6648, val loss 2.6874\n",
      "step 3960: train loss 2.6484, val loss 2.6784\n",
      "step 3980: train loss 2.6496, val loss 2.6748\n",
      "step 4000: train loss 2.6595, val loss 2.6829\n",
      "step 4020: train loss 2.6472, val loss 2.6679\n",
      "step 4040: train loss 2.6428, val loss 2.6671\n",
      "step 4060: train loss 2.6528, val loss 2.6511\n",
      "step 4080: train loss 2.6313, val loss 2.6486\n",
      "step 4100: train loss 2.6434, val loss 2.6533\n",
      "step 4120: train loss 2.6469, val loss 2.6523\n",
      "step 4140: train loss 2.6375, val loss 2.6591\n",
      "step 4160: train loss 2.6376, val loss 2.6507\n",
      "step 4180: train loss 2.6315, val loss 2.6538\n",
      "step 4200: train loss 2.6310, val loss 2.6471\n",
      "step 4220: train loss 2.6233, val loss 2.6428\n",
      "step 4240: train loss 2.6347, val loss 2.6453\n",
      "step 4260: train loss 2.6201, val loss 2.6354\n",
      "step 4280: train loss 2.6155, val loss 2.6310\n",
      "step 4300: train loss 2.6275, val loss 2.6378\n",
      "step 4320: train loss 2.6255, val loss 2.6296\n",
      "step 4340: train loss 2.6095, val loss 2.6392\n",
      "step 4360: train loss 2.6188, val loss 2.6211\n",
      "step 4380: train loss 2.6078, val loss 2.6308\n",
      "step 4400: train loss 2.6061, val loss 2.6177\n",
      "step 4420: train loss 2.6076, val loss 2.6254\n",
      "step 4440: train loss 2.6063, val loss 2.6081\n",
      "step 4460: train loss 2.6115, val loss 2.6186\n",
      "step 4480: train loss 2.6047, val loss 2.6163\n",
      "step 4500: train loss 2.5991, val loss 2.6236\n",
      "step 4520: train loss 2.6079, val loss 2.6175\n",
      "step 4540: train loss 2.5935, val loss 2.6235\n",
      "step 4560: train loss 2.5981, val loss 2.6075\n",
      "step 4580: train loss 2.5974, val loss 2.6223\n",
      "step 4600: train loss 2.5946, val loss 2.6065\n",
      "step 4620: train loss 2.5837, val loss 2.6020\n",
      "step 4640: train loss 2.5911, val loss 2.6051\n",
      "step 4660: train loss 2.5930, val loss 2.5981\n",
      "step 4680: train loss 2.5945, val loss 2.5997\n",
      "step 4700: train loss 2.5886, val loss 2.6134\n",
      "step 4720: train loss 2.5851, val loss 2.6030\n",
      "step 4740: train loss 2.5841, val loss 2.6078\n",
      "step 4760: train loss 2.5799, val loss 2.5924\n",
      "step 4780: train loss 2.5922, val loss 2.5925\n",
      "step 4800: train loss 2.5912, val loss 2.5927\n",
      "step 4820: train loss 2.5911, val loss 2.5824\n",
      "step 4840: train loss 2.5696, val loss 2.5811\n",
      "step 4860: train loss 2.5684, val loss 2.5934\n",
      "step 4880: train loss 2.5773, val loss 2.5911\n",
      "step 4900: train loss 2.5815, val loss 2.5882\n",
      "step 4920: train loss 2.5753, val loss 2.5785\n",
      "step 4940: train loss 2.5709, val loss 2.5789\n",
      "step 4960: train loss 2.5678, val loss 2.5894\n",
      "step 4980: train loss 2.5764, val loss 2.5872\n",
      "step 5000: train loss 2.5658, val loss 2.5812\n",
      "step 5020: train loss 2.5726, val loss 2.5756\n",
      "step 5040: train loss 2.5599, val loss 2.5761\n",
      "step 5060: train loss 2.5756, val loss 2.5727\n",
      "step 5080: train loss 2.5608, val loss 2.5862\n",
      "step 5100: train loss 2.5557, val loss 2.5703\n",
      "step 5120: train loss 2.5688, val loss 2.5734\n",
      "step 5140: train loss 2.5539, val loss 2.5620\n",
      "step 5160: train loss 2.5517, val loss 2.5732\n",
      "step 5180: train loss 2.5566, val loss 2.5808\n",
      "step 5200: train loss 2.5668, val loss 2.5785\n",
      "step 5220: train loss 2.5558, val loss 2.5644\n",
      "step 5240: train loss 2.5510, val loss 2.5544\n",
      "step 5260: train loss 2.5596, val loss 2.5640\n",
      "step 5280: train loss 2.5550, val loss 2.5611\n",
      "step 5300: train loss 2.5349, val loss 2.5775\n",
      "step 5320: train loss 2.5460, val loss 2.5740\n",
      "step 5340: train loss 2.5582, val loss 2.5538\n",
      "step 5360: train loss 2.5557, val loss 2.5622\n",
      "step 5380: train loss 2.5511, val loss 2.5664\n",
      "step 5400: train loss 2.5421, val loss 2.5710\n",
      "step 5420: train loss 2.5512, val loss 2.5559\n",
      "step 5440: train loss 2.5359, val loss 2.5636\n",
      "step 5460: train loss 2.5375, val loss 2.5608\n",
      "step 5480: train loss 2.5450, val loss 2.5667\n",
      "step 5500: train loss 2.5365, val loss 2.5470\n",
      "step 5520: train loss 2.5428, val loss 2.5535\n",
      "step 5540: train loss 2.5330, val loss 2.5644\n",
      "step 5560: train loss 2.5460, val loss 2.5494\n",
      "step 5580: train loss 2.5330, val loss 2.5444\n",
      "step 5600: train loss 2.5391, val loss 2.5611\n",
      "step 5620: train loss 2.5281, val loss 2.5434\n",
      "step 5640: train loss 2.5448, val loss 2.5633\n",
      "step 5660: train loss 2.5350, val loss 2.5584\n",
      "step 5680: train loss 2.5455, val loss 2.5460\n",
      "step 5700: train loss 2.5361, val loss 2.5508\n",
      "step 5720: train loss 2.5259, val loss 2.5456\n",
      "step 5740: train loss 2.5197, val loss 2.5427\n",
      "step 5760: train loss 2.5404, val loss 2.5421\n",
      "step 5780: train loss 2.5220, val loss 2.5298\n",
      "step 5800: train loss 2.5365, val loss 2.5445\n",
      "step 5820: train loss 2.5316, val loss 2.5460\n",
      "step 5840: train loss 2.5324, val loss 2.5486\n",
      "step 5860: train loss 2.5350, val loss 2.5478\n",
      "step 5880: train loss 2.5257, val loss 2.5358\n",
      "step 5900: train loss 2.5297, val loss 2.5411\n",
      "step 5920: train loss 2.5319, val loss 2.5458\n",
      "step 5940: train loss 2.5200, val loss 2.5464\n",
      "step 5960: train loss 2.5302, val loss 2.5242\n",
      "step 5980: train loss 2.5284, val loss 2.5339\n",
      "step 6000: train loss 2.5390, val loss 2.5479\n",
      "step 6020: train loss 2.5173, val loss 2.5258\n",
      "step 6040: train loss 2.5279, val loss 2.5341\n",
      "step 6060: train loss 2.5130, val loss 2.5201\n",
      "step 6080: train loss 2.5229, val loss 2.5340\n",
      "step 6100: train loss 2.5242, val loss 2.5355\n",
      "step 6120: train loss 2.5171, val loss 2.5519\n",
      "step 6140: train loss 2.5124, val loss 2.5296\n",
      "step 6160: train loss 2.5068, val loss 2.5171\n",
      "step 6180: train loss 2.5120, val loss 2.5259\n",
      "step 6200: train loss 2.5127, val loss 2.5297\n",
      "step 6220: train loss 2.5142, val loss 2.5366\n",
      "step 6240: train loss 2.5148, val loss 2.5356\n",
      "step 6260: train loss 2.5085, val loss 2.5383\n",
      "step 6280: train loss 2.5099, val loss 2.5142\n",
      "step 6300: train loss 2.5234, val loss 2.5337\n",
      "step 6320: train loss 2.5348, val loss 2.5208\n",
      "step 6340: train loss 2.5178, val loss 2.5320\n",
      "step 6360: train loss 2.5184, val loss 2.5225\n",
      "step 6380: train loss 2.5166, val loss 2.5233\n",
      "step 6400: train loss 2.5149, val loss 2.5313\n",
      "step 6420: train loss 2.5174, val loss 2.5350\n",
      "step 6440: train loss 2.5070, val loss 2.5192\n",
      "step 6460: train loss 2.5067, val loss 2.5333\n",
      "step 6480: train loss 2.4960, val loss 2.5261\n",
      "step 6500: train loss 2.5027, val loss 2.5225\n",
      "step 6520: train loss 2.5059, val loss 2.5207\n",
      "step 6540: train loss 2.5146, val loss 2.5278\n",
      "step 6560: train loss 2.4996, val loss 2.5211\n",
      "step 6580: train loss 2.5173, val loss 2.5115\n",
      "step 6600: train loss 2.4989, val loss 2.5090\n",
      "step 6620: train loss 2.5130, val loss 2.5277\n",
      "step 6640: train loss 2.5052, val loss 2.5207\n",
      "step 6660: train loss 2.5158, val loss 2.5257\n",
      "step 6680: train loss 2.5117, val loss 2.5153\n",
      "step 6700: train loss 2.5068, val loss 2.5133\n",
      "step 6720: train loss 2.5071, val loss 2.5167\n",
      "step 6740: train loss 2.4927, val loss 2.5193\n",
      "step 6760: train loss 2.5098, val loss 2.5091\n",
      "step 6780: train loss 2.5015, val loss 2.5157\n",
      "step 6800: train loss 2.5042, val loss 2.5201\n",
      "step 6820: train loss 2.5025, val loss 2.5252\n",
      "step 6840: train loss 2.5044, val loss 2.5221\n",
      "step 6860: train loss 2.5107, val loss 2.5245\n",
      "step 6880: train loss 2.4954, val loss 2.5193\n",
      "step 6900: train loss 2.5034, val loss 2.5212\n",
      "step 6920: train loss 2.5044, val loss 2.5101\n",
      "step 6940: train loss 2.4865, val loss 2.5236\n",
      "step 6960: train loss 2.4983, val loss 2.5043\n",
      "step 6980: train loss 2.4997, val loss 2.5069\n",
      "step 7000: train loss 2.5092, val loss 2.5156\n",
      "step 7020: train loss 2.5057, val loss 2.5241\n",
      "step 7040: train loss 2.4969, val loss 2.5137\n",
      "step 7060: train loss 2.4856, val loss 2.5240\n",
      "step 7080: train loss 2.5009, val loss 2.5030\n",
      "step 7100: train loss 2.4965, val loss 2.5105\n",
      "step 7120: train loss 2.4824, val loss 2.5077\n",
      "step 7140: train loss 2.4953, val loss 2.5155\n",
      "step 7160: train loss 2.4969, val loss 2.5128\n",
      "step 7180: train loss 2.4934, val loss 2.5155\n",
      "step 7200: train loss 2.4937, val loss 2.5149\n",
      "step 7220: train loss 2.4934, val loss 2.5194\n",
      "step 7240: train loss 2.4893, val loss 2.5193\n",
      "step 7260: train loss 2.4957, val loss 2.5117\n",
      "step 7280: train loss 2.4917, val loss 2.5182\n",
      "step 7300: train loss 2.4938, val loss 2.5011\n",
      "step 7320: train loss 2.4845, val loss 2.5139\n",
      "step 7340: train loss 2.4859, val loss 2.5043\n",
      "step 7360: train loss 2.4890, val loss 2.4988\n",
      "step 7380: train loss 2.4862, val loss 2.5152\n",
      "step 7400: train loss 2.4817, val loss 2.5005\n",
      "step 7420: train loss 2.4965, val loss 2.5131\n",
      "step 7440: train loss 2.4988, val loss 2.5142\n",
      "step 7460: train loss 2.4915, val loss 2.5158\n",
      "step 7480: train loss 2.4869, val loss 2.5118\n",
      "step 7500: train loss 2.4784, val loss 2.5061\n",
      "step 7520: train loss 2.4840, val loss 2.5008\n",
      "step 7540: train loss 2.4920, val loss 2.4941\n",
      "step 7560: train loss 2.4972, val loss 2.5090\n",
      "step 7580: train loss 2.4918, val loss 2.4981\n",
      "step 7600: train loss 2.4907, val loss 2.5150\n",
      "step 7620: train loss 2.4799, val loss 2.5033\n",
      "step 7640: train loss 2.4926, val loss 2.4991\n",
      "step 7660: train loss 2.4842, val loss 2.5021\n",
      "step 7680: train loss 2.4933, val loss 2.5163\n",
      "step 7700: train loss 2.4887, val loss 2.5069\n",
      "step 7720: train loss 2.4850, val loss 2.5014\n",
      "step 7740: train loss 2.4906, val loss 2.5055\n",
      "step 7760: train loss 2.4864, val loss 2.5141\n",
      "step 7780: train loss 2.4832, val loss 2.5061\n",
      "step 7800: train loss 2.4862, val loss 2.5007\n",
      "step 7820: train loss 2.4894, val loss 2.5047\n",
      "step 7840: train loss 2.4912, val loss 2.5029\n",
      "step 7860: train loss 2.4903, val loss 2.5085\n",
      "step 7880: train loss 2.4944, val loss 2.5030\n",
      "step 7900: train loss 2.4862, val loss 2.5062\n",
      "step 7920: train loss 2.4789, val loss 2.5048\n",
      "step 7940: train loss 2.4780, val loss 2.5036\n",
      "step 7960: train loss 2.4932, val loss 2.5115\n",
      "step 7980: train loss 2.4862, val loss 2.5016\n",
      "step 8000: train loss 2.4908, val loss 2.4998\n",
      "step 8020: train loss 2.4827, val loss 2.5009\n",
      "step 8040: train loss 2.4774, val loss 2.5045\n",
      "step 8060: train loss 2.4905, val loss 2.5061\n",
      "step 8080: train loss 2.4756, val loss 2.4948\n",
      "step 8100: train loss 2.4842, val loss 2.5060\n",
      "step 8120: train loss 2.4812, val loss 2.4931\n",
      "step 8140: train loss 2.4884, val loss 2.4982\n",
      "step 8160: train loss 2.4767, val loss 2.5012\n",
      "step 8180: train loss 2.4797, val loss 2.4993\n",
      "step 8200: train loss 2.4935, val loss 2.5086\n",
      "step 8220: train loss 2.4768, val loss 2.4914\n",
      "step 8240: train loss 2.4819, val loss 2.4932\n",
      "step 8260: train loss 2.4836, val loss 2.4952\n",
      "step 8280: train loss 2.4776, val loss 2.4977\n",
      "step 8300: train loss 2.4804, val loss 2.4820\n",
      "step 8320: train loss 2.4848, val loss 2.5032\n",
      "step 8340: train loss 2.4771, val loss 2.4775\n",
      "step 8360: train loss 2.4842, val loss 2.5041\n",
      "step 8380: train loss 2.4686, val loss 2.4954\n",
      "step 8400: train loss 2.4713, val loss 2.5017\n",
      "step 8420: train loss 2.4767, val loss 2.5026\n",
      "step 8440: train loss 2.4770, val loss 2.4976\n",
      "step 8460: train loss 2.4723, val loss 2.4975\n",
      "step 8480: train loss 2.4879, val loss 2.5020\n",
      "step 8500: train loss 2.4849, val loss 2.4937\n",
      "step 8520: train loss 2.4775, val loss 2.5099\n",
      "step 8540: train loss 2.4671, val loss 2.4979\n",
      "step 8560: train loss 2.4725, val loss 2.4934\n",
      "step 8580: train loss 2.4649, val loss 2.4866\n",
      "step 8600: train loss 2.4822, val loss 2.4998\n",
      "step 8620: train loss 2.4737, val loss 2.5002\n",
      "step 8640: train loss 2.4710, val loss 2.4957\n",
      "step 8660: train loss 2.4803, val loss 2.4912\n",
      "step 8680: train loss 2.4664, val loss 2.4899\n",
      "step 8700: train loss 2.4655, val loss 2.4920\n",
      "step 8720: train loss 2.4827, val loss 2.4973\n",
      "step 8740: train loss 2.4863, val loss 2.4927\n",
      "step 8760: train loss 2.4702, val loss 2.4927\n",
      "step 8780: train loss 2.4919, val loss 2.5039\n",
      "step 8800: train loss 2.4756, val loss 2.4979\n",
      "step 8820: train loss 2.4744, val loss 2.4890\n",
      "step 8840: train loss 2.4611, val loss 2.4876\n",
      "step 8860: train loss 2.4690, val loss 2.4897\n",
      "step 8880: train loss 2.4554, val loss 2.5008\n",
      "step 8900: train loss 2.4657, val loss 2.4911\n",
      "step 8920: train loss 2.4672, val loss 2.4941\n",
      "step 8940: train loss 2.4827, val loss 2.4773\n",
      "step 8960: train loss 2.4700, val loss 2.4900\n",
      "step 8980: train loss 2.4818, val loss 2.4907\n",
      "step 9000: train loss 2.4736, val loss 2.4874\n",
      "step 9020: train loss 2.4710, val loss 2.4950\n",
      "step 9040: train loss 2.4663, val loss 2.4929\n",
      "step 9060: train loss 2.4699, val loss 2.5028\n",
      "step 9080: train loss 2.4798, val loss 2.5041\n",
      "step 9100: train loss 2.4677, val loss 2.4981\n",
      "step 9120: train loss 2.4657, val loss 2.4866\n",
      "step 9140: train loss 2.4553, val loss 2.4895\n",
      "step 9160: train loss 2.4800, val loss 2.4983\n",
      "step 9180: train loss 2.4699, val loss 2.4918\n",
      "step 9200: train loss 2.4718, val loss 2.4909\n",
      "step 9220: train loss 2.4667, val loss 2.4963\n",
      "step 9240: train loss 2.4661, val loss 2.4842\n",
      "step 9260: train loss 2.4654, val loss 2.4967\n",
      "step 9280: train loss 2.4633, val loss 2.4978\n",
      "step 9300: train loss 2.4788, val loss 2.4782\n",
      "step 9320: train loss 2.4775, val loss 2.4981\n",
      "step 9340: train loss 2.4766, val loss 2.5048\n",
      "step 9360: train loss 2.4727, val loss 2.4872\n",
      "step 9380: train loss 2.4770, val loss 2.4872\n",
      "step 9400: train loss 2.4636, val loss 2.4818\n",
      "step 9420: train loss 2.4731, val loss 2.4934\n",
      "step 9440: train loss 2.4636, val loss 2.4875\n",
      "step 9460: train loss 2.4732, val loss 2.4875\n",
      "step 9480: train loss 2.4668, val loss 2.5034\n",
      "step 9500: train loss 2.4687, val loss 2.4926\n",
      "step 9520: train loss 2.4637, val loss 2.4878\n",
      "step 9540: train loss 2.4642, val loss 2.5033\n",
      "step 9560: train loss 2.4711, val loss 2.4910\n",
      "step 9580: train loss 2.4671, val loss 2.4930\n",
      "step 9600: train loss 2.4762, val loss 2.4985\n",
      "step 9620: train loss 2.4668, val loss 2.4739\n",
      "step 9640: train loss 2.4681, val loss 2.4950\n",
      "step 9660: train loss 2.4686, val loss 2.4867\n",
      "step 9680: train loss 2.4804, val loss 2.4880\n",
      "step 9700: train loss 2.4657, val loss 2.4905\n",
      "step 9720: train loss 2.4808, val loss 2.4890\n",
      "step 9740: train loss 2.4567, val loss 2.5061\n",
      "step 9760: train loss 2.4716, val loss 2.4907\n",
      "step 9780: train loss 2.4703, val loss 2.4770\n",
      "step 9800: train loss 2.4747, val loss 2.5029\n",
      "step 9820: train loss 2.4583, val loss 2.4954\n",
      "step 9840: train loss 2.4641, val loss 2.4864\n",
      "step 9860: train loss 2.4720, val loss 2.4962\n",
      "step 9880: train loss 2.4706, val loss 2.4825\n",
      "step 9900: train loss 2.4566, val loss 2.4952\n",
      "step 9920: train loss 2.4811, val loss 2.4860\n",
      "step 9940: train loss 2.4632, val loss 2.4839\n",
      "step 9960: train loss 2.4686, val loss 2.4896\n",
      "step 9980: train loss 2.4659, val loss 2.4976\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUCARDUK:\n",
      "Burrelam kenstr'd nt ay, an't th f en ane be tof f s, ard eaint-be.\n",
      "PUR:Por ghy.\n",
      "AMUE:\n",
      "vee\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
