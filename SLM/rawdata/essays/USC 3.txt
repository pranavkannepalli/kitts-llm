My fascination with natural language processing (NLP) began in 9th grade while playing with Google Translate. It seemed simple on the surface—input a sentence, and out comes a translation in one of 234 languages. Yet, I couldn’t stop thinking about how this works. Language is so full of nuance, and yet machines manage to break it down and process it. That curiosity set me on a path to understand how we can model language mathematically, resonating with my love for both math and coding.
This connection between math and language processing pushed me to seek out mentorship from Krishna Chintalapudi at Microsoft. Together, we explored classification techniques using large language models (LLMs), and I gained insight into the complexities of modeling human expression. As I dug deeper, I also realized the limitations of current NLP tools—despite there being over 8,000 languages, most digital platforms only support a fraction of them. This sparked my desire to work on including low-resource languages in NLP, making sure more voices are represented globally.
In this context, the NAE Grand Challenge of "Reverse Engineering the Brain" is most important to me. Understanding the brain's mechanisms for language comprehension could revolutionize NLP by mimicking human cognition. It would allow us to build AI that not only translates languages but captures subtleties like tone and cultural context. This would also support the inclusion of low-resource languages, bridging global communication gaps. By reverse-engineering the brain’s language processing capabilities, we can make technology more inclusive and accessible, truly engineering a better world for all.