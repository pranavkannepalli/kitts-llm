I remember the first time I grasped the power of natural language processing (NLP). In 9th grade, I was toying around with Google Translate when I began wondering how it works. To an end user, it might look simple—type in a sentence, hit enter, and voilà, the machine gives you a translation for any of its 234 languages. I couldn’t help but think, How is this even possible? Language is so messy and full of nuances.
This fascination with language processing resonated with my love for math, which has always been a lens through which I view the world. I often find myself searching for numerical patterns, whether it’s factoring the numbers on street signs or playing “24” with words (where A=1, B=2, and so on). Similarly, NLP utilizes machine translation, which employs statistical models and algorithms to convert language into numerical representations that a machine can understand, before processing it into a different language.
This led me to seek the mentorship of Krishna Chintalapudi, a Principal Researcher at Microsoft, with whom I worked on classification techniques using large language models (LLMs). This experience opened my eyes to the intricacies of language processing and the potential to model the subtleties of human expression.
However, as I explored NLP, I encountered a staggering reality: while there are 8,324 recorded languages, Google Translate only supports 234. I aspire to develop techniques that incorporate low-resource languages into digital platforms, enhancing accessibility and ensuring diverse voices participate in the global conversation.