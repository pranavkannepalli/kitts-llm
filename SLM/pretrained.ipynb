{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranavk/Documents/github/kitts-llm/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-05 13:55:32.849874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736114132.861105  207488 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736114132.864614  207488 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-05 13:55:32.878167: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128\n",
    "stride = 1\n",
    "embedding_dim = 512\n",
    "num_layers = 24\n",
    "num_heads = 16\n",
    "ff_dim = 1024\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I remember the first time I grasped the power of natural language processing (NLP). In 9th grade, I was toying around with Google Translate when I began wondering how it works. To an end user, it might look simple—type in a sentence, hit enter, and voilà, the machine gives you a translation for any of its 234 languages. I couldn\\'t help but think, How is this even possible? Language is so messy and full of nuances.\\nThis fascination resonated with my love for math—I find myself searching for numerical patterns, whether factoring street numbers or playing \"24\" with words (where A=1, B=2, and so on). Through NLP, I discovered how computers could transform language into mathematical representations, combining my love of patterns with real-world impact.\\nMy passion grew through hands-on projects. After completing Stanford\\'s NLP course on Coursera, I sought mentorship from Krishna Chintalapudi, a Principal Researcher at Microsoft, to improve text classification techniques. For Technology Student Association competitions, I integrated NLP into real-world applications, including a sustainable initiatives website with AI-powered suggestions for homeowners. I also developed an app that helps students find college and internship matches using ChatGPT to generate personalized lessons and tasks.\\nBut what truly cemented my decision to major in Computer Science was discovering NLP\\'s limitations: while over 8,000 languages exist worldwide, these tools support only a small fraction. Through my work with language models, I\\'ve seen how computer science can transform communication, but this transformation remains inaccessible to many communities, particularly those speaking low-resource languages. \\nThus, I plan to use my Computer Science education to found a startup extending NLP capabilities to low-resource languages, making digital communication tools truly inclusive. For me, CS isn\\'t just a major—it\\'s a path to ensuring no community gets left behind in our digital future.',\n",
       " 'Math was my first love, enthralling me with its evolving nature and pushing me to think critically. This passion led to Computer Science, where I discovered a similar thrill, with the added benefit of creating solutions that impact people’s lives. While working for a nonprofit in India, I built a website for low-income women to sell their products. I had to quickly learn how to set up Google Cloud service accounts, create translation layers for accessibility, and develop a backend using Google Sheets as a database, ensuring it was user-friendly for a team with no coding experience. Acquiring these new skills, though challenging, was worth it when I witnessed the website reaching a broader audience and allowing these women to gain financial independence. It also gave me a clear understanding of how I want to use technology in the future—to empower people and create lasting change.\\nCraving a deeper understanding, I pursued research under a Principal Researcher at Microsoft, exploring NLP classification techniques using LLMs. This forced me to think beyond code, diving into the training mechanisms and optimizations behind these models. Being exposed to the depth of CS, I realized how much there remains to learn about the technology behind the solutions I create.\\nI further honed my skills during an internship at Applied Systems, where I worked as the only high schooler on their AI engineering team and took on the responsibilities of a full software engineer. I was entrusted with building a test playground in React to help clients evaluate their services. To achieve this, I worked with production APIs and deployment manifests, written in languages I knew little about. I turned to my colleagues for support, who guided me with the fundamentals of Go and Kubernetes. Each new challenge reinforced that coding is about constant learning and adaptation—skills I’ve embraced from the start.\\nAt CMU, I will continue pushing the boundaries of what I can create.',\n",
       " \"For me, a successful college experience means acquiring the skills and knowledge to make meaningful contributions to the field of AI while creating impactful solutions for underrepresented communities. Carnegie Mellon University’s rigorous academic environment and focus on innovation make it the ideal place to realize this vision.\\nCMU’s robust computer science program excites me, especially courses like 11-711: Algorithms for NLP and 11-744: Question Answering and Dialogue Systems, which align with my passion for low-resource language processing. These courses will help me build upon my high school NLP research and internship experiences, where I developed classification techniques for LLMs. At CMU, I aspire to expand these efforts by working with faculty like Professor Graham Neubig, whose research on multilingual NLP and transfer learning for low-resource languages aligns with my goal of bridging language barriers. His work inspires me to explore how NLP can empower communities worldwide, whether through improved translation systems or accessible AI tools.\\nBeyond coursework, CMU’s focus on interdisciplinary collaboration is deeply appealing. The Language Technologies Institute and its emphasis on both theoretical and applied AI will provide me with opportunities to engage in cutting-edge research. I’m particularly drawn to the CMU AI Mentorship Program, where I can collaborate with peers and mentors on impactful projects. This will prepare me for my ultimate goal of founding an AI startup focused on low-resource languages, leveraging CMU's Swartz Center for Entrepreneurship for guidance and resources.\\nA successful college experience is also about balance and personal growth. CMU’s vibrant cricket culture offers the perfect outlet for me to stay active and connect with fellow students. Competing with international peers will remind me of the resilience and teamwork I’ve cultivated through years of playing cricket. At CMU, I hope to merge technical excellence with personal fulfillment, equipping myself to leave a lasting impact on the world.\",\n",
       " \"I come from a community rooted in STEM, where learning and teaching go hand in hand. As a student, I've challenged myself through advanced courses, national-level math competitions, and self-studying coding languages, preparing me for internships and real-world applications. But my passion extends beyond personal growth; I'm committed to sharing knowledge with those who lack equal access, which inspired me to found the Kirkland chapter of Steel City Codes (SCC), offering free coding lessons to underrepresented kids.\\nInitially, my lessons didn't resonate – I was wrongly assuming students had prior tech knowledge. After recognizing this oversight, I revamped the curriculum with interactive elements and trained volunteers to adapt lessons for varying skill levels. Starting with simple analogies for beginners, like explaining variables as boxes holding information, while offering challenge problems for advanced students, we saw transformative results. One student, Tommaso, went from reluctant participant to eager learner, while Mateo progressed from struggling with basic commands to developing his own text-based adventure game. Their journey from confusion to enthusiasm became my most rewarding experience.\\nAt CMU, I'm especially excited to contribute to CMU CS Academy, a platform I've experienced firsthand as a student. Having benefited from its well-structured Python curriculum, I'm eager to help enhance and develop new content that can reach even more students nationwide. Through the Leonard Gelfand Center's K-12 outreach initiatives, I plan to create additional support systems for schools adopting CS Academy, drawing from my experience with both sides of the platform to make computer science education more accessible and engaging.\\nAdditionally, I hope to work with CMU's Computer Science Pathways program, which connects local high school students with undergraduate mentors. By sharing my experiences and knowledge, I can help inspire the next generation of Pittsburgh's tech innovators while contributing to CMU's mission of fostering inclusive technical education in our community.\",\n",
       " 'As much as I’m a student, I’m also a teacher. This belief inspired me to found the Kirkland chapter of Steel City Codes (SCC) to teach coding to underrepresented children. \\nWith SCC, I developed a curriculum that emphasizes hands-on learning through engaging projects and designed workshops that introduce fundamental programming concepts, allowing students to create their own applications and games. By fostering an interactive learning environment, I encouraged creativity and critical thinking while making coding accessible to all participants. Moreover, I collaborated with local schools and community organizations to reach a diverse range of students, ensuring that everyone has the opportunity to explore the tech field.\\nTeaching has not only enabled me to share my knowledge but has also deepened my understanding. I’m eager to continue this work at Columbia through programs like Sci-Inspire and Columbia University Competitions in Math, mentoring young learners and making STEM more inclusive.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_essays_from_folder(folder_path):\n",
    "    essays = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                essays.append(f.read())\n",
    "    return essays \n",
    "\n",
    "essays = load_essays_from_folder('rawdata/essays')\n",
    "essays[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sliding_window_data(sequence):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(0, len(sequence) - window_size, stride):\n",
    "        window_sequence = sequence[i:i + window_size]\n",
    "        next_tokens = sequence[i + 1:i + window_size + 1]  # Shift by one for labels\n",
    "        inputs.append(window_sequence)\n",
    "        labels.append(next_tokens)\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(essays):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for essay in essays:\n",
    "        tokenized_essay = tokenizer(\n",
    "                                    essay,\n",
    "                                    padding=\"max_length\",\n",
    "                                    return_tensors=\"np\",\n",
    "                                )[\"input_ids\"][0]\n",
    "        essay_inputs, essay_labels = generate_sliding_window_data(tokenized_essay)\n",
    "        inputs.extend(essay_inputs)\n",
    "        labels.extend(essay_labels)\n",
    "\n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels= prepare_data(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94080, 128), (94080, 128))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittuSLMData(Dataset):\n",
    "    def __init__(self, tokenized_inputs, tokenized_labels):\n",
    "        self.input_ids = tokenized_inputs\n",
    "        self.labels = tokenized_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = KittuSLMData(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75264, 18816)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split(dataset, [train_Size, val_size])\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranavk/Documents/github/kitts-llm/venv/lib64/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/4704 [00:33<25:41,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9669, 'grad_norm': 1.2045801877975464, 'learning_rate': 4.8937074829931974e-05, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 200/4704 [01:07<25:42,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5589, 'grad_norm': 1.6572766304016113, 'learning_rate': 4.7874149659863945e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 300/4704 [01:41<25:58,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4172, 'grad_norm': 1.857711672782898, 'learning_rate': 4.6811224489795916e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 400/4704 [02:15<24:19,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2911, 'grad_norm': 2.28802752494812, 'learning_rate': 4.5748299319727895e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 500/4704 [02:49<23:43,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2061, 'grad_norm': 1.130794644355774, 'learning_rate': 4.4685374149659866e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 600/4704 [03:23<24:02,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.177, 'grad_norm': 1.2035126686096191, 'learning_rate': 4.362244897959184e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 700/4704 [03:58<23:34,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1453, 'grad_norm': 0.6960482597351074, 'learning_rate': 4.255952380952381e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 800/4704 [04:31<22:08,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1058, 'grad_norm': 1.1275807619094849, 'learning_rate': 4.149659863945579e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 900/4704 [05:06<21:12,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0699, 'grad_norm': 1.9548039436340332, 'learning_rate': 4.043367346938776e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 1000/4704 [05:40<20:20,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0685, 'grad_norm': 0.26908838748931885, 'learning_rate': 3.937074829931973e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1100/4704 [06:13<19:59,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0721, 'grad_norm': 0.5885570049285889, 'learning_rate': 3.83078231292517e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1200/4704 [06:47<19:37,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0615, 'grad_norm': 0.39010483026504517, 'learning_rate': 3.724489795918368e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1300/4704 [07:21<19:08,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.053, 'grad_norm': 1.0771703720092773, 'learning_rate': 3.618197278911565e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1400/4704 [07:54<18:13,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.045, 'grad_norm': 0.8651258945465088, 'learning_rate': 3.511904761904762e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1500/4704 [08:28<17:52,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0426, 'grad_norm': 1.138550043106079, 'learning_rate': 3.405612244897959e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1600/4704 [09:03<17:32,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0408, 'grad_norm': 0.6409257650375366, 'learning_rate': 3.2993197278911564e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1700/4704 [09:37<16:42,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0378, 'grad_norm': 0.7477817535400391, 'learning_rate': 3.193027210884354e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1800/4704 [10:11<16:19,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0351, 'grad_norm': 0.5872397422790527, 'learning_rate': 3.086734693877551e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1900/4704 [10:44<15:06,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0403, 'grad_norm': 0.9978460073471069, 'learning_rate': 2.9804421768707485e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 2000/4704 [11:18<15:32,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0386, 'grad_norm': 0.5194665789604187, 'learning_rate': 2.8741496598639456e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 2100/4704 [11:52<14:49,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0354, 'grad_norm': 0.6175212264060974, 'learning_rate': 2.767857142857143e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 2200/4704 [12:25<13:31,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0366, 'grad_norm': 0.5463506579399109, 'learning_rate': 2.6615646258503402e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2300/4704 [12:58<12:54,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0282, 'grad_norm': 0.6239235401153564, 'learning_rate': 2.5552721088435377e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 2400/4704 [13:31<12:38,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0313, 'grad_norm': 1.7174289226531982, 'learning_rate': 2.448979591836735e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 2500/4704 [14:04<12:01,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.031, 'grad_norm': 0.46412011981010437, 'learning_rate': 2.342687074829932e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2600/4704 [14:37<11:32,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0278, 'grad_norm': 0.6976783275604248, 'learning_rate': 2.2363945578231294e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 2700/4704 [15:09<11:12,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0273, 'grad_norm': 0.00035124149871990085, 'learning_rate': 2.1301020408163266e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 2800/4704 [15:42<10:13,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0281, 'grad_norm': 0.4442143440246582, 'learning_rate': 2.023809523809524e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 2900/4704 [16:14<09:46,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.028, 'grad_norm': 0.47989848256111145, 'learning_rate': 1.9175170068027212e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3000/4704 [16:47<09:15,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0293, 'grad_norm': 0.5651704668998718, 'learning_rate': 1.8112244897959187e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 3100/4704 [17:20<08:37,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0293, 'grad_norm': 0.7169039845466614, 'learning_rate': 1.7049319727891158e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3200/4704 [17:53<08:14,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0277, 'grad_norm': 0.21078425645828247, 'learning_rate': 1.5986394557823133e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3300/4704 [18:25<07:35,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0278, 'grad_norm': 0.6674048900604248, 'learning_rate': 1.4923469387755104e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 3400/4704 [18:58<07:21,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0243, 'grad_norm': 0.9188097715377808, 'learning_rate': 1.3860544217687074e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 3500/4704 [19:31<06:34,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0261, 'grad_norm': 0.53313148021698, 'learning_rate': 1.2797619047619047e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 3600/4704 [20:03<05:54,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.025, 'grad_norm': 0.6938477158546448, 'learning_rate': 1.1734693877551021e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 3700/4704 [20:36<05:21,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0257, 'grad_norm': 0.617882251739502, 'learning_rate': 1.0671768707482993e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 3800/4704 [21:09<05:01,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.024, 'grad_norm': 0.6341880559921265, 'learning_rate': 9.608843537414966e-06, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 3900/4704 [21:41<04:20,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0255, 'grad_norm': 0.3389674425125122, 'learning_rate': 8.545918367346939e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 4000/4704 [22:14<03:49,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.026, 'grad_norm': 0.5443024039268494, 'learning_rate': 7.482993197278912e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 4100/4704 [22:47<03:15,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0253, 'grad_norm': 0.6722580194473267, 'learning_rate': 6.420068027210885e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 4200/4704 [23:19<02:42,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.024, 'grad_norm': 0.48607105016708374, 'learning_rate': 5.357142857142857e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 4300/4704 [23:52<02:17,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0237, 'grad_norm': 0.00017042089893948287, 'learning_rate': 4.29421768707483e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 4400/4704 [24:25<01:39,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0249, 'grad_norm': 0.4028944969177246, 'learning_rate': 3.231292517006803e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 4500/4704 [24:57<01:05,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0244, 'grad_norm': 0.375034362077713, 'learning_rate': 2.1683673469387757e-06, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 4600/4704 [25:30<00:33,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0243, 'grad_norm': 0.45057255029678345, 'learning_rate': 1.1054421768707483e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4700/4704 [26:03<00:01,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0225, 'grad_norm': 0.3041726052761078, 'learning_rate': 4.251700680272109e-08, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 4704/4704 [28:20<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023786062374711037, 'eval_runtime': 134.2979, 'eval_samples_per_second': 140.106, 'eval_steps_per_second': 8.757, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "100%|██████████| 4704/4704 [28:30<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1710.7934, 'train_samples_per_second': 43.994, 'train_steps_per_second': 2.75, 'train_loss': 0.0894479629057808, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4704, training_loss=0.0894479629057808, metrics={'train_runtime': 1710.7934, 'train_samples_per_second': 43.994, 'train_steps_per_second': 2.75, 'total_flos': 4914872743624704.0, 'train_loss': 0.0894479629057808, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember the first time I grasped the power of natural language processing (NLP). In 9th grade, I was toying around with Google Translate when I began wondering how it works. an an, anof,of isofd that, an conducted, turned, took place Google a,,,,a,,,,,,,,,,,,,,,,,,,,,,,, Googlelate,., an a where, was it an about whether to or not.in an inth, inth, at, place I to, that my showed, me the, that I the, that me the I create\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I remember the first time I grasped the power of natural language processing (NLP). In 9th grade, I was toying around with Google Translate when I began wondering how it works.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=256,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
